{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7434728-82c9-4e67-b282-e6d0fa7eaffd",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90bcdf-810d-4616-93d9-01c5d38911e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirments.txt OR !pip install ultralytics opencv-python numpy matplotlib tqdm ipython jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b1438-b408-4373-bbdd-9998a46a0285",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8560fde-6be2-418e-8c13-9e6d0358adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports and Setup\n",
    "import cv2\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import time # 引入 time 模組\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "# torch.classes.__path__ = [] # Potentially not needed in Jupyter, but kept if specific env issues arise\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tqdm.notebook import tqdm # For progress bar in Jupyter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Video, clear_output, HTML # For displaying video and richer output\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = 'yolov8n.pt'  # You can choose different YOLOv8 models\n",
    "TARGET_CLASSES = [0, 2] # COCO dataset: 0: person, 2: car\n",
    "TARGET_CLASS_NAMES = {0: 'Person', 2: 'Car'} # For displaying class names\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "TRACKER_CONFIG = 'bytetrack.yaml' # Make sure this file exists or Ultralytics can fetch it\n",
    "\n",
    "# --- Bounding Box Drawing Parameters ---\n",
    "BOX_COLOR = (0, 255, 0) # BGR format green\n",
    "BOX_THICKNESS = 2\n",
    "TEXT_COLOR = (0, 255, 0) # Green, but will be changed to white on green background\n",
    "TEXT_FONT_SCALE = 0.5\n",
    "TEXT_THICKNESS = 1\n",
    "TEXT_FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# --- Jupyter Specific Output Control ---\n",
    "MAX_REPRESENTATIVE_FRAMES_DISPLAY = 12 # Max representative frames to show at once\n",
    "FRAMES_PER_ROW_REPRESENTATIVE = 4\n",
    "MAX_SELECTED_TRACK_FRAMES_DISPLAY = 20 # Max frames to show for a selected track\n",
    "FRAMES_PER_ROW_SELECTED = 5\n",
    "\n",
    "# %% Helper Functions\n",
    "\n",
    "# No caching decorator needed for simple script, model loaded once.\n",
    "# If you were calling load_model multiple times with same path, functools.lru_cache could be used.\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load YOLO model\"\"\"\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "        print(f\"Model '{model_path}' loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def draw_bounding_box(frame, box, track_id, class_name):\n",
    "    \"\"\"Draw a single bounding box and label on the given frame.\"\"\"\n",
    "    img_with_box = frame.copy() # Create a copy to avoid modifying the original frame\n",
    "    x1, y1, x2, y2 = map(int, box) # Ensure coordinates are integers\n",
    "    label = f'ID:{track_id} {class_name}'\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(img_with_box, (x1, y1), (x2, y2), BOX_COLOR, BOX_THICKNESS)\n",
    "\n",
    "    # Draw label background\n",
    "    (w, h), _ = cv2.getTextSize(label, TEXT_FONT, TEXT_FONT_SCALE, TEXT_THICKNESS)\n",
    "    label_y = y1 - 10 if y1 - 10 > h else y1 + h + 10 # Prevent label from going off screen top\n",
    "    cv2.rectangle(img_with_box, (x1, label_y - h - 5), (x1 + w, label_y), BOX_COLOR, -1) # -1 for filled\n",
    "\n",
    "    # Draw label text (white on green background for better visibility)\n",
    "    cv2.putText(img_with_box, label, (x1, label_y - 3), TEXT_FONT, TEXT_FONT_SCALE, (255, 255, 255), TEXT_THICKNESS, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return img_with_box\n",
    "\n",
    "def process_video(video_path, model):\n",
    "    \"\"\"\n",
    "    Process video, detect and track objects, store frames and corresponding bounding box info.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tracked_object_frames, representative_frames)\n",
    "               tracked_object_frames: dict[int, list[tuple(np.ndarray, tuple)]]\n",
    "                   - Stores list of (original_frame, box_coordinates) for each track_id\n",
    "               representative_frames: dict[int, tuple(np.ndarray, str, tuple)]\n",
    "                   - Stores (original_frame, class_name, box_coordinates) for each track_id\n",
    "    \"\"\"\n",
    "    tracked_object_frames = defaultdict(list)\n",
    "    representative_frames = {}\n",
    "    # object_classes = {} # Not explicitly used later for now, but could be useful\n",
    "\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video file.\")\n",
    "            return {}, {}\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Initialize tqdm progress bar\n",
    "        pbar = tqdm(total=total_frames, desc=\"Processing video\")\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            fps = frame_count / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            pbar.set_postfix_str(f\"FPS: {fps:.2f}\")\n",
    "\n",
    "            # Use YOLO model for tracking\n",
    "            results = model.track(\n",
    "                source=frame,\n",
    "                tracker=TRACKER_CONFIG,\n",
    "                classes=TARGET_CLASSES,\n",
    "                conf=CONFIDENCE_THRESHOLD,\n",
    "                persist=True, # Keep tracks between frames\n",
    "                verbose=False # Suppress Ultralytics output for cleaner console\n",
    "            )\n",
    "\n",
    "            if results and results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "                boxes = results[0].boxes.xyxy.cpu().numpy() # Keep as float for potential precision\n",
    "                track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "                class_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "                for box, track_id, cls_id in zip(boxes, track_ids, class_ids):\n",
    "                    # Store the original frame and box coordinates for this object\n",
    "                    tracked_object_frames[track_id].append((frame.copy(), tuple(box)))\n",
    "\n",
    "                    # If this is the first time seeing this track_id, store its representative frame info\n",
    "                    if track_id not in representative_frames:\n",
    "                        class_name = TARGET_CLASS_NAMES.get(cls_id, f'Class {cls_id}')\n",
    "                        representative_frames[track_id] = (frame.copy(), class_name, tuple(box))\n",
    "                        # object_classes[track_id] = cls_id\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        pbar.close()\n",
    "        print(\"Video processing complete!\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_processing_time = end_time - start_time\n",
    "        avg_fps = total_frames / total_processing_time if total_processing_time > 0 else 0\n",
    "        print(f\"Total frames: {total_frames}, Processed in: {total_processing_time:.2f}s, Average FPS: {avg_fps:.2f}\")\n",
    "\n",
    "\n",
    "        return tracked_object_frames, representative_frames\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during video processing: {e}\")\n",
    "        if 'cap' in locals() and cap.isOpened():\n",
    "            cap.release()\n",
    "        if 'pbar' in locals():\n",
    "            pbar.close()\n",
    "        return {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb20340-7650-42b6-b7d1-16730261bc5d",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9b61f-7b9f-4d87-9e50-670198129b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Main Script Execution ---\n",
    "\n",
    "# --- 1. Define Video Path ---\n",
    "# << IMPORTANT >>: Replace this with the actual path to your video file\n",
    "VIDEO_FILE_PATH = \"video/3105211-sd_640_360_30fps.mp4\" # e.g., \"test_video.mp4\" or \"/path/to/your/video.mov\"\n",
    "\n",
    "if not os.path.exists(VIDEO_FILE_PATH):\n",
    "    print(f\"ERROR: Video file not found at '{VIDEO_FILE_PATH}'. Please set the correct path.\")\n",
    "    # You might want to stop execution here if the file isn't found\n",
    "    # For Jupyter, an error message is usually enough, or raise FileNotFoundError\n",
    "else:\n",
    "    print(f\"Attempting to process video: {VIDEO_FILE_PATH}\")\n",
    "    # Display the video in Jupyter (optional)\n",
    "    # display(Video(VIDEO_FILE_PATH, embed=True, width=400)) # Smaller width for display\n",
    "\n",
    "    # --- 2. Load Model ---\n",
    "    yolo_model = load_model(MODEL_PATH)\n",
    "\n",
    "    tracked_data = None\n",
    "    representative_frames_data = None\n",
    "\n",
    "    if yolo_model:\n",
    "        # --- 3. Process Video ---\n",
    "        # This is where the main processing happens\n",
    "        print(\"\\nStarting video processing...\")\n",
    "        tracked_data, representative_frames_data = process_video(VIDEO_FILE_PATH, yolo_model)\n",
    "    else:\n",
    "        print(\"Model could not be loaded. Aborting video processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2377688-7df9-4e12-ae08-00c5c6418b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Display Results ---\n",
    "if representative_frames_data:\n",
    "    print(f\"\\n--- Tracked Object Results ---\")\n",
    "    print(f\"Detected and tracked {len(representative_frames_data)} unique objects (Persons or Cars).\")\n",
    "\n",
    "    print(\"\\n--- Representative Frame for Each Object (with Bounding Box) ---\")\n",
    "    \n",
    "    track_ids_list = list(representative_frames_data.keys())\n",
    "    num_objects = len(track_ids_list)\n",
    "    \n",
    "    if num_objects == 0:\n",
    "        print(\"No objects detected or tracked.\")\n",
    "    else:\n",
    "        # Limit the number of representative frames displayed directly in the notebook\n",
    "        num_to_display = min(num_objects, MAX_REPRESENTATIVE_FRAMES_DISPLAY)\n",
    "        if num_objects > MAX_REPRESENTATIVE_FRAMES_DISPLAY:\n",
    "             print(f\"Displaying first {MAX_REPRESENTATIVE_FRAMES_DISPLAY} out of {num_objects} detected objects.\")\n",
    "        \n",
    "        cols = FRAMES_PER_ROW_REPRESENTATIVE\n",
    "        rows = (num_to_display + cols - 1) // cols # Calculate rows needed\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "        axes = axes.flatten() # Flatten to 1D array for easy indexing\n",
    "\n",
    "        for i in range(num_to_display):\n",
    "            track_id = track_ids_list[i]\n",
    "            rep_frame_bgr, class_name, box = representative_frames_data[track_id]\n",
    "            \n",
    "            frame_with_box = draw_bounding_box(rep_frame_bgr, box, track_id, class_name)\n",
    "            rep_frame_rgb = cv2.cvtColor(frame_with_box, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            ax = axes[i]\n",
    "            ax.imshow(rep_frame_rgb)\n",
    "            ax.set_title(f\"Object ID: {track_id} ({class_name})\")\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for j in range(num_to_display, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef890d7-7de3-4f7a-9bfe-c12dc89f44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Display All Frames for a CHOSEN Object ---\n",
    "print(\"\\n--- All Frames for a Selected Object ---\")\n",
    "\n",
    "# <<< IMPORTANT >>>: Set this to the ID you want to inspect from the output above\n",
    "# If no specific ID, it will try the first one if available.\n",
    "CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES = 1 \n",
    "\n",
    "if not CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES and track_ids_list:\n",
    "    CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES = track_ids_list[0] # Default to the first track ID\n",
    "    print(f\"No specific track ID chosen. Displaying frames for the first tracked object: ID {CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES}\")\n",
    "elif not track_ids_list:\n",
    "     print(\"No objects were tracked, so no specific frames to display.\")\n",
    "     CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES = None\n",
    "\n",
    "\n",
    "if CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES is not None and tracked_data:\n",
    "    selected_id = CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES\n",
    "    if selected_id in tracked_data:\n",
    "        frames_data_to_show = tracked_data[selected_id]\n",
    "        \n",
    "        _rep_frame, class_name_selected, _rep_box = representative_frames_data.get(selected_id, (None, \"Unknown\", None))\n",
    "\n",
    "        print(f\"\\nDisplaying frames for Object ID: {selected_id} ({class_name_selected}) - Total {len(frames_data_to_show)} frames.\")\n",
    "\n",
    "        num_frames_selected = len(frames_data_to_show)\n",
    "        num_to_display_selected = min(num_frames_selected, MAX_SELECTED_TRACK_FRAMES_DISPLAY)\n",
    "\n",
    "        if num_frames_selected > MAX_SELECTED_TRACK_FRAMES_DISPLAY:\n",
    "            print(f\"This object appeared in {num_frames_selected} frames. Displaying the first {MAX_SELECTED_TRACK_FRAMES_DISPLAY}.\")\n",
    "        \n",
    "        if num_to_display_selected > 0:\n",
    "            cols_sel = FRAMES_PER_ROW_SELECTED\n",
    "            rows_sel = (num_to_display_selected + cols_sel - 1) // cols_sel\n",
    "            \n",
    "            fig_sel, axes_sel = plt.subplots(rows_sel, cols_sel, figsize=(cols_sel * 3, rows_sel * 3))\n",
    "            if rows_sel == 1 and cols_sel == 1: # Handle single plot case\n",
    "                 axes_sel = np.array([axes_sel])\n",
    "            axes_sel = axes_sel.flatten()\n",
    "\n",
    "            for idx, frame_data in enumerate(frames_data_to_show[:num_to_display_selected]):\n",
    "                frame_bgr, box = frame_data\n",
    "                # For simplicity, just draw a rectangle. For full label use draw_bounding_box\n",
    "                frame_to_display = frame_bgr.copy()\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame_to_display, (x1, y1), (x2, y2), BOX_COLOR, BOX_THICKNESS)\n",
    "                \n",
    "                # If you want the full label on each frame:\n",
    "                # frame_to_display = draw_bounding_box(frame_bgr, box, selected_id, class_name_selected)\n",
    "\n",
    "\n",
    "                frame_rgb = cv2.cvtColor(frame_to_display, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                ax = axes_sel[idx]\n",
    "                ax.imshow(frame_rgb)\n",
    "                ax.set_title(f\"Frame {idx+1}\")\n",
    "                ax.axis('off')\n",
    "            \n",
    "            for j in range(num_to_display_selected, len(axes_sel)):\n",
    "                fig_sel.delaxes(axes_sel[j])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No frames to display for object ID {selected_id}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Could not find data for Track ID {selected_id}.\")\n",
    "elif CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES is not None:\n",
    "     print(f\"Track ID {CHOOSE_TRACK_ID_TO_VIEW_ALL_FRAMES} was specified, but no tracking data is available (tracked_data is empty or None).\")\n",
    "\n",
    "\n",
    "# --- Footer ---\n",
    "print(\"\\n---\")\n",
    "display(HTML(\"<em>Powered by Ultralytics YOLOv8 and Jupyter</em>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
