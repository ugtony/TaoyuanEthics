import cv2
import tempfile
import os

import torch
torch.classes.__path__ = [] # æŠŠ torch.classes è£¡é¢çš„ __path__ æ¸…æ‰ï¼ŒStreamlit å°±ç„¡è·¯å¯èµ°ã€ä¹Ÿä¸æœƒæ‹‹ä¾‹å¤–

import streamlit as st
from ultralytics import YOLO
import numpy as np
from collections import defaultdict
import time # å¼•å…¥ time æ¨¡çµ„

# --- è¨­å®š ---
MODEL_PATH = 'yolov8n.pt'  # æ‚¨å¯ä»¥é¸æ“‡ä¸åŒçš„ YOLOv8 æ¨¡å‹
TARGET_CLASSES = [0, 2] # COCO è³‡æ–™é›†ä¸­: 0: person, 2: car
TARGET_CLASS_NAMES = {0: 'Person', 2: 'Car'} # æ–¹ä¾¿é¡¯ç¤ºé¡åˆ¥åç¨±
CONFIDENCE_THRESHOLD = 0.3
TRACKER_CONFIG = 'bytetrack.yaml'
BOX_COLOR = (0, 255, 0) # BGR æ ¼å¼çš„ç¶ è‰²
BOX_THICKNESS = 2
TEXT_COLOR = (0, 255, 0)
TEXT_FONT_SCALE = 0.5
TEXT_THICKNESS = 1
TEXT_FONT = cv2.FONT_HERSHEY_SIMPLEX

# --- Helper å‡½å¼ ---

@st.cache_resource # å¿«å–æ¨¡å‹è¼‰å…¥
def load_model(model_path):
    """è¼‰å…¥ YOLO æ¨¡å‹"""
    try:
        model = YOLO(model_path)
        return model
    except Exception as e:
        st.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
        return None

def draw_bounding_box(frame, box, track_id, class_name):
    """åœ¨æŒ‡å®šçš„å¹€ä¸Šç¹ªè£½å–®ä¸€ç‰©ä»¶çš„æ–¹æ¡†å’Œæ¨™ç±¤"""
    img_with_box = frame.copy() # å»ºç«‹å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹å¹€
    x1, y1, x2, y2 = map(int, box) # ç¢ºä¿åº§æ¨™æ˜¯æ•´æ•¸
    label = f'ID:{track_id} {class_name}'

    # ç¹ªè£½æ–¹æ¡†
    cv2.rectangle(img_with_box, (x1, y1), (x2, y2), BOX_COLOR, BOX_THICKNESS)

    # ç¹ªè£½æ¨™ç±¤èƒŒæ™¯
    (w, h), _ = cv2.getTextSize(label, TEXT_FONT, TEXT_FONT_SCALE, TEXT_THICKNESS)
    label_y = y1 - 10 if y1 - 10 > h else y1 + h + 10 # é˜²æ­¢æ¨™ç±¤è¶…å‡ºåœ–ç‰‡é ‚éƒ¨
    cv2.rectangle(img_with_box, (x1, label_y - h - 5), (x1 + w, label_y), BOX_COLOR, -1) # -1 è¡¨ç¤ºå¡«æ»¿

    # ç¹ªè£½æ¨™ç±¤æ–‡å­— (ç™½è‰²å­—é«”åœ¨ç¶ è‰²èƒŒæ™¯ä¸Šæ›´æ¸…æ™°)
    cv2.putText(img_with_box, label, (x1, label_y - 3), TEXT_FONT, TEXT_FONT_SCALE, (255, 255, 255), TEXT_THICKNESS, lineType=cv2.LINE_AA)

    return img_with_box


def process_video(video_path, model):
    """
    è™•ç†å½±ç‰‡ï¼Œåµæ¸¬ä¸¦è¿½è¹¤ç‰©ä»¶ï¼Œå„²å­˜å¹€å’Œå°æ‡‰çš„æ–¹æ¡†è³‡è¨Šã€‚

    Returns:
        tuple: (tracked_object_frames, representative_frames)
               tracked_object_frames: dict[int, list[tuple(np.ndarray, tuple)]]
                   - å„²å­˜æ¯å€‹ track_id å°æ‡‰çš„ (åŸå§‹å¹€, æ–¹æ¡†åº§æ¨™) åˆ—è¡¨
               representative_frames: dict[int, tuple(np.ndarray, str, tuple)]
                   - å„²å­˜æ¯å€‹ track_id çš„ (åŸå§‹å¹€, é¡åˆ¥åç¨±, æ–¹æ¡†åº§æ¨™)
    """
    tracked_object_frames = defaultdict(list)
    representative_frames = {}
    object_classes = {}

    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            st.error("ç„¡æ³•é–‹å•Ÿå½±ç‰‡æª”æ¡ˆã€‚")
            return {}, {}

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        progress_bar = st.progress(0, text="æ­£åœ¨è™•ç†å½±ç‰‡...")
        frame_count = 0
        start_time = time.time() # é–‹å§‹è¨ˆæ™‚

        while True:
            success, frame = cap.read()
            if not success:
                break

            frame_count += 1
            current_time = time.time()
            elapsed_time = current_time - start_time
            fps = frame_count / elapsed_time if elapsed_time > 0 else 0

            # ä½¿ç”¨ YOLO æ¨¡å‹é€²è¡Œè¿½è¹¤
            results = model.track(
                source=frame,
                tracker=TRACKER_CONFIG,
                classes=TARGET_CLASSES,
                conf=CONFIDENCE_THRESHOLD,
                persist=True,
                verbose=False
            )

            if results and results[0].boxes is not None and results[0].boxes.id is not None:
                boxes = results[0].boxes.xyxy.cpu().numpy() # ä¿ç•™ float æ–¹ä¾¿å¾ŒçºŒè™•ç†
                track_ids = results[0].boxes.id.cpu().numpy().astype(int)
                class_ids = results[0].boxes.cls.cpu().numpy().astype(int)

                for box, track_id, cls_id in zip(boxes, track_ids, class_ids):
                    # å„²å­˜é€™å€‹ç‰©ä»¶çš„åŸå§‹å¹€å’Œæ–¹æ¡†åº§æ¨™
                    tracked_object_frames[track_id].append((frame.copy(), tuple(box))) # å„²å­˜å¹€å‰¯æœ¬å’Œ box

                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡çœ‹åˆ°é€™å€‹ track_idï¼Œå„²å­˜ä»£è¡¨å¹€è³‡è¨Š
                    if track_id not in representative_frames:
                        class_name = TARGET_CLASS_NAMES.get(cls_id, f'Class {cls_id}')
                        representative_frames[track_id] = (frame.copy(), class_name, tuple(box)) # å„²å­˜å¹€å‰¯æœ¬, åç¨±, box
                        object_classes[track_id] = cls_id

            # æ›´æ–°é€²åº¦æ¢å’Œ FPS
            progress_percent = int((frame_count / total_frames) * 100) if total_frames > 0 else 0
            progress_bar.progress(progress_percent, text=f"è™•ç†ä¸­... {progress_percent}% (FPS: {fps:.2f})")

        cap.release()
        progress_bar.progress(100, text="å½±ç‰‡è™•ç†å®Œæˆï¼")

        return tracked_object_frames, representative_frames

    except Exception as e:
        st.error(f"è™•ç†å½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        if 'cap' in locals() and cap.isOpened():
            cap.release()
        return {}, {}
    finally:
        if 'progress_bar' in locals():
            progress_bar.empty()


# --- Streamlit ä»‹é¢ ---

st.set_page_config(page_title="YOLO ç‰©ä»¶è¿½è¹¤ App", layout="wide")
st.title("ğŸ¬ YOLOv8 ç‰©ä»¶åµæ¸¬èˆ‡è¿½è¹¤å±•ç¤º (å«æ–¹æ¡†æ¨™ç¤º)")
st.write(f"ä½¿ç”¨ **{MODEL_PATH}** æ¨¡å‹åµæ¸¬ **äººå“¡ (Person)** å’Œ **æ±½è»Š (Car)**ï¼Œä¸¦åœ¨ç•«é¢ä¸Šæ¨™ç¤ºç‰©ä»¶ã€‚")
st.write("æ³¨æ„ï¼šå½±ç‰‡è™•ç†å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ã€‚")

# åˆå§‹åŒ– session state
if 'tracked_data' not in st.session_state:
    st.session_state.tracked_data = None
if 'representative_frames' not in st.session_state:
    st.session_state.representative_frames = None
if 'selected_track_id' not in st.session_state:
    st.session_state.selected_track_id = None
if 'video_processed' not in st.session_state:
    st.session_state.video_processed = False
if 'uploaded_file_name' not in st.session_state:
    st.session_state.uploaded_file_name = None


# --- æª”æ¡ˆä¸Šå‚³ ---
uploaded_file = st.file_uploader("è«‹é¸æ“‡ä¸€å€‹å½±ç‰‡æª”æ¡ˆ", type=["mp4", "avi", "mov", "mkv"])

if uploaded_file is not None:
    if not st.session_state.uploaded_file_name or st.session_state.uploaded_file_name != uploaded_file.name:
        st.session_state.tracked_data = None
        st.session_state.representative_frames = None
        st.session_state.selected_track_id = None
        st.session_state.video_processed = False
        st.session_state.uploaded_file_name = uploaded_file.name
        if 'video_path' in st.session_state:
            # å˜—è©¦æ¸…ç†èˆŠçš„æš«å­˜æª”
            path_to_clean = st.session_state.get('video_path')
            if path_to_clean and os.path.exists(path_to_clean):
                 try:
                     os.remove(path_to_clean)
                 except Exception:
                     pass # å¿½ç•¥æ¸…ç†éŒ¯èª¤
            del st.session_state['video_path']


    video_path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            video_path = tmp_file.name
            st.session_state['video_path'] = video_path

        st.video(video_path)

        model = load_model(MODEL_PATH)

        if model and not st.session_state.video_processed:
            if st.button("ğŸš€ é–‹å§‹è™•ç†å½±ç‰‡", key="process_btn"):
                with st.spinner(f'æ¨¡å‹ ({MODEL_PATH}) æ­£åœ¨åŠªåŠ›å·¥ä½œä¸­ï¼Œè«‹ç¨å€™...'):
                    st.session_state.tracked_data = None
                    st.session_state.representative_frames = None
                    st.session_state.selected_track_id = None

                    tracked_data, representative_frames = process_video(video_path, model)

                    st.session_state.tracked_data = tracked_data
                    st.session_state.representative_frames = representative_frames
                    st.session_state.video_processed = True
                    st.rerun()

    finally:
        # é€™è£¡ä¸å†éœ€è¦æ¸…ç† video_pathï¼Œå› ç‚ºæœƒåœ¨ä¸‹æ¬¡ä¸Šå‚³æˆ–ç¨‹åºçµæŸæ™‚ç”± OS æ¸…ç† delete=False çš„æš«å­˜æ–‡ä»¶
        # å¦‚æœå¸Œæœ›æ›´ä¸»å‹•æ¸…ç†ï¼Œéœ€è¦æ›´è¤‡é›œçš„ç‹€æ…‹ç®¡ç†
        pass


# --- çµæœé¡¯ç¤º ---
if st.session_state.video_processed and st.session_state.representative_frames:
    st.header("ğŸ“Š è¿½è¹¤ç‰©ä»¶çµæœ")
    st.write(f"å…±åµæ¸¬ä¸¦è¿½è¹¤åˆ° {len(st.session_state.representative_frames)} å€‹ç¨ç«‹ç‰©ä»¶ (äººå“¡æˆ–æ±½è»Š)ã€‚")

    st.subheader("æ¯å€‹ç‰©ä»¶çš„ä»£è¡¨ç•«é¢ (å·²æ¨™ç¤ºæ–¹æ¡†)")
    st.write("é»æ“Šç‰©ä»¶æ—çš„æŒ‰éˆ•æŸ¥çœ‹è©²ç‰©ä»¶çš„æ‰€æœ‰ç•«é¢ã€‚")

    num_cols = 4 # èª¿æ•´æ¯è¡Œé¡¯ç¤ºæ•¸é‡
    cols = st.columns(num_cols)
    rep_frames_data = st.session_state.representative_frames
    track_ids = list(rep_frames_data.keys())

    for i, track_id in enumerate(track_ids):
        col_index = i % num_cols
        with cols[col_index]:
            # *** æ›´æ–°é» 1: å–å¾—å¹€ã€é¡åˆ¥åã€æ–¹æ¡†åº§æ¨™ä¸¦ç¹ªè£½ ***
            rep_frame_bgr, class_name, box = rep_frames_data[track_id]
            frame_with_box = draw_bounding_box(rep_frame_bgr, box, track_id, class_name)
            rep_frame_rgb = cv2.cvtColor(frame_with_box, cv2.COLOR_BGR2RGB)

            st.image(rep_frame_rgb, caption=f"ç‰©ä»¶ ID: {track_id} ({class_name})", use_container_width=True)
            button_key = f"view_btn_{track_id}"
            if st.button(f"æŸ¥çœ‹ ID:{track_id} æ‰€æœ‰ç•«é¢", key=button_key):
                st.session_state.selected_track_id = track_id
                st.rerun()

    st.markdown("---")

    if st.session_state.selected_track_id is not None:
        selected_id = st.session_state.selected_track_id
        if selected_id in st.session_state.tracked_data:
            frames_data_to_show = st.session_state.tracked_data[selected_id]
            if selected_id in st.session_state.representative_frames:
                _, class_name, _ = st.session_state.representative_frames[selected_id]
                st.subheader(f"ğŸ–¼ï¸ ç‰©ä»¶ ID: {selected_id} ({class_name}) çš„æ‰€æœ‰ç•«é¢ ({len(frames_data_to_show)} å¹€)")
            else:
                 st.subheader(f"ğŸ–¼ï¸ ç‰©ä»¶ ID: {selected_id} çš„æ‰€æœ‰ç•«é¢ ({len(frames_data_to_show)} å¹€)")

            max_frames_display = 50
            display_data = frames_data_to_show
            if len(frames_data_to_show) > max_frames_display:
                st.warning(f"æ­¤ç‰©ä»¶å‡ºç¾è¶…é {max_frames_display} å¹€ï¼Œåƒ…é¡¯ç¤ºå‰ {max_frames_display} å¹€ã€‚")
                display_data = frames_data_to_show[:max_frames_display]

            frame_cols = st.columns(4)
            for idx, frame_data in enumerate(display_data):
                with frame_cols[idx % 4]:
                    # *** æ›´æ–°é» 2: å–å¾—å¹€å’Œæ–¹æ¡†åº§æ¨™ä¸¦ç¹ªè£½ ***
                    frame_bgr, box = frame_data
                    # åœ¨é€™è£¡æˆ‘å€‘åªéœ€è¦æ¡†ï¼Œå¯ä»¥ä¸åŠ  ID å’Œé¡åˆ¥æ¨™ç±¤ï¼Œé¿å…ç•«é¢æ··äº‚
                    frame_to_display = frame_bgr.copy()
                    x1, y1, x2, y2 = map(int, box)
                    cv2.rectangle(frame_to_display, (x1, y1), (x2, y2), BOX_COLOR, BOX_THICKNESS)

                    frame_rgb = cv2.cvtColor(frame_to_display, cv2.COLOR_BGR2RGB)
                    st.image(frame_rgb, caption=f"å¹€ {idx+1}", use_container_width=True)
        else:
            st.warning(f"æ‰¾ä¸åˆ° Track ID {selected_id} çš„è³‡æ–™ã€‚")
            st.session_state.selected_track_id = None

elif st.session_state.video_processed and not st.session_state.representative_frames:
    st.info("å½±ç‰‡è™•ç†å®Œæˆï¼Œä½†æœªåµæ¸¬æˆ–è¿½è¹¤åˆ°ä»»ä½•æŒ‡å®šçš„ç‰©ä»¶ã€‚")

# --- é è…³ ---
st.markdown("---")
st.caption("ç”± Ultralytics YOLOv8 å’Œ Streamlit é©…å‹•")